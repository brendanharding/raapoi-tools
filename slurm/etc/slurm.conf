# THIS FILE IS CONTROLLED BY ELASTICLUSTER
# local modifications will be overwritten
# the next time `elasticluster setup` is run!
## **Note:** This file needs to have identical contents on all nodes of
# the cluster.  See the `slurm.conf` man page for more information.
#

# Unique name for identifying this cluster entries in the DB
#raapoi cluster
ClusterName=raapoi


## Cluster nodes
#
# c03n01
NodeName=c03n01 RealMemory=63877 CPUs=24 Sockets=2 CoresPerSocket=6 ThreadsPerCore=2 Feature=AMD,IB
# c03n02
NodeName=c03n02 RealMemory=63877 CPUs=24 Sockets=2 CoresPerSocket=6 ThreadsPerCore=2 Feature=AMD,IB
# c03n03
NodeName=c03n03 RealMemory=63877 CPUs=24 Sockets=2 CoresPerSocket=6 ThreadsPerCore=2 Feature=AMD,IB
# c03n04
NodeName=c03n04 RealMemory=63877 CPUs=24 Sockets=2 CoresPerSocket=6 ThreadsPerCore=2 Feature=AMD,IB
# c04n01
NodeName=c04n01 RealMemory=128128 CPUs=64 Sockets=2 CoresPerSocket=16 ThreadsPerCore=2 Feature=Intel,IB 
# c04n02
NodeName=c04n02 RealMemory=128128 CPUs=64 Sockets=2 CoresPerSocket=16 ThreadsPerCore=2 Feature=Intel,IB 
# c04n03
NodeName=c04n03 RealMemory=128128 CPUs=64 Sockets=2 CoresPerSocket=16 ThreadsPerCore=2 Feature=Intel,IB 
# c04n04
NodeName=c04n04 RealMemory=128128 CPUs=64 Sockets=2 CoresPerSocket=16 ThreadsPerCore=2 Feature=Intel,IB 
# c05n01
NodeName=c05n01 RealMemory=63877 CPUs=24 Sockets=2 CoresPerSocket=6 ThreadsPerCore=2 Feature=AMD,IB 
# c05n02
NodeName=c05n02 RealMemory=63877 CPUs=24 Sockets=2 CoresPerSocket=6 ThreadsPerCore=2 Feature=AMD,IB 
# c05n04
NodeName=c05n04 RealMemory=63877 CPUs=24 Sockets=2 CoresPerSocket=6 ThreadsPerCore=2 Feature=AMD,IB 
# c06n01
NodeName=c06n01 RealMemory=63877 CPUs=24 Sockets=2 CoresPerSocket=6 ThreadsPerCore=2 Feature=AMD,IB 
# c06n02
NodeName=c06n02 RealMemory=63877 CPUs=24 Sockets=2 CoresPerSocket=6 ThreadsPerCore=2 Feature=AMD,IB 
# c06n03
NodeName=c06n03 RealMemory=63877 CPUs=24 Sockets=2 CoresPerSocket=6 ThreadsPerCore=2 Feature=AMD,IB 
# c06n04
NodeName=c06n04 RealMemory=63877 CPUs=24 Sockets=2 CoresPerSocket=6 ThreadsPerCore=2 Feature=AMD,IB 
# c07n01
NodeName=c07n01 RealMemory=63860 CPUs=32 Sockets=2 CoresPerSocket=8 ThreadsPerCore=2 Feature=AMD,IB 
# c07n03
NodeName=c07n03 RealMemory=63860 CPUs=32 Sockets=2 CoresPerSocket=8 ThreadsPerCore=2 Feature=AMD,IB 
# c07n04
NodeName=c07n04 RealMemory=63860 CPUs=32 Sockets=2 CoresPerSocket=8 ThreadsPerCore=2 Feature=AMD,IB 
# c08n01
NodeName=c08n01 RealMemory=63860 CPUs=32 Sockets=2 CoresPerSocket=8 ThreadsPerCore=2 Feature=AMD,IB 
# c08n02
NodeName=c08n02 RealMemory=63860 CPUs=32 Sockets=2 CoresPerSocket=8 ThreadsPerCore=2 Feature=AMD,IB 
# c08n03
NodeName=c08n03 RealMemory=63860 CPUs=32 Sockets=2 CoresPerSocket=8 ThreadsPerCore=2 Feature=AMD,IB 
# c08n04
NodeName=c08n04 RealMemory=63860 CPUs=32 Sockets=2 CoresPerSocket=8 ThreadsPerCore=2 Feature=AMD,IB 
# c09n01
NodeName=c09n01 RealMemory=63768 CPUs=48 Sockets=2 CoresPerSocket=12 ThreadsPerCore=2 Feature=Intel,IB 
# c09n02
NodeName=c09n02 RealMemory=63768 CPUs=48 Sockets=2 CoresPerSocket=12 ThreadsPerCore=2 Feature=Intel,IB 
# c09n03
NodeName=c09n03 RealMemory=63768 CPUs=48 Sockets=2 CoresPerSocket=12 ThreadsPerCore=2 Feature=Intel,IB 
# c09n04
NodeName=c09n04 RealMemory=63768 CPUs=48 Sockets=2 CoresPerSocket=12 ThreadsPerCore=2 Feature=Intel,IB 
# c10n01
NodeName=c10n01 RealMemory=515428 CPUs=48 Sockets=2 CoresPerSocket=12 ThreadsPerCore=2 Feature=AMD,IB 
# c11n01
NodeName=c11n01 RealMemory=1031390 CPUs=40 Sockets=2 CoresPerSocket=10 ThreadsPerCore=2 Feature=Intel,IB 
# c12n01
NodeName=c12n01 RealMemory=128268 CPUs=64 Sockets=2 CoresPerSocket=16 ThreadsPerCore=2 Feature=Intel,IB 
# c12n02
NodeName=c12n02 RealMemory=128268 CPUs=64 Sockets=2 CoresPerSocket=16 ThreadsPerCore=2 Feature=Intel,IB 
# c12n03
NodeName=c12n03 RealMemory=128268 CPUs=64 Sockets=2 CoresPerSocket=16 ThreadsPerCore=2 Feature=Intel,IB 
# c12n04
NodeName=c12n04 RealMemory=128268 CPUs=64 Sockets=2 CoresPerSocket=16 ThreadsPerCore=2 Feature=Intel,IB 
# c13n01
NodeName=c13n01 RealMemory=128268 CPUs=64 Sockets=2 CoresPerSocket=16 ThreadsPerCore=2 Feature=Intel,IB 
# c13n02
NodeName=c13n02 RealMemory=128268 CPUs=64 Sockets=2 CoresPerSocket=16 ThreadsPerCore=2 Feature=Intel,IB 
# c13n03
NodeName=c13n03 RealMemory=128268 CPUs=64 Sockets=2 CoresPerSocket=16 ThreadsPerCore=2 Feature=Intel,IB 
# c13n04
NodeName=c13n04 RealMemory=128268 CPUs=64 Sockets=2 CoresPerSocket=16 ThreadsPerCore=2 Feature=Intel,IB 


# cad01
NodeName=cad01 RealMemory=130401 CPUs=24 Sockets=24 CoresPerSocket=1 ThreadsPerCore=1 Feature=Intel,10GE,10GigE 
# cad02
NodeName=cad02 RealMemory=130401 CPUs=24 Sockets=24 CoresPerSocket=1 ThreadsPerCore=1 Feature=Intel,10GE,10GigE 
# cad03
NodeName=cad03 RealMemory=130401 CPUs=24 Sockets=24 CoresPerSocket=1 ThreadsPerCore=1 Feature=Intel,10GE,10GigE 
# cad04
NodeName=cad04 RealMemory=130401 CPUs=24 Sockets=24 CoresPerSocket=1 ThreadsPerCore=1 Feature=Intel,10GE,10GigE 
# cad05
NodeName=cad05 RealMemory=130401 CPUs=24 Sockets=24 CoresPerSocket=1 ThreadsPerCore=1 Feature=Intel,10GE,10GigE 
# cad06
NodeName=cad06 RealMemory=130402 CPUs=16 Sockets=16 CoresPerSocket=1 ThreadsPerCore=1 Feature=Intel,10GE,10GigE 
# cad07
NodeName=cad07 RealMemory=130401 CPUs=24 Sockets=24 CoresPerSocket=1 ThreadsPerCore=1 Feature=Intel,10GE,10GigE 
# cad08
NodeName=cad08 RealMemory=130401 CPUs=24 Sockets=24 CoresPerSocket=1 ThreadsPerCore=1 Feature=Intel,10GE,10GigE 
# cadhigh01
NodeName=cadhigh01 RealMemory=515316 CPUs=48 Sockets=48 CoresPerSocket=1 ThreadsPerCore=1 Feature=Intel,10GE,10GigE 
# cadhigh02
NodeName=cadhigh02 RealMemory=515316 CPUs=48 Sockets=48 CoresPerSocket=1 ThreadsPerCore=1 Feature=Intel,10GE,10GigE 
# cadhigh03
NodeName=cadhigh03 RealMemory=515316 CPUs=48 Sockets=48 CoresPerSocket=1 ThreadsPerCore=1 Feature=Intel,10GE,10GigE 
# cadhigh04
NodeName=cadhigh04 RealMemory=515316 CPUs=48 Sockets=48 CoresPerSocket=1 ThreadsPerCore=1 Feature=Intel,10GE,10GigE 


## Cluster partition
#
#raapoi partitions
PartitionName=quicktest Nodes=c03n01, MaxTime=01:00:00 State=UP Default=YES DefaultTime=01:00:00
PartitionName=bigmem Nodes=c10n01,c11n01,cadhigh01,cadhigh02,cadhigh03,cadhigh04, MaxTime=10-00:00 State=UP Default=NO DefaultTime=01:00:00
PartitionName=parallel Nodes=c03n02,c03n03,c03n04,c04n01,c04n02,c04n03,c04n04,c05n01,c05n02,c05n04,c06n01,c06n02,c06n03,c06n04,c07n01,c07n03,c07n04,c08n01,c08n02,c08n03,c08n04,c09n01,c09n02,c09n03,c09n04,c12n01,c12n02,c12n03,c12n04,c13n01,c13n02,c13n03,c13n04,cad01,cad02,cad03,cad04,cad05,cad06,cad07,cad08, MaxTime=10-00:00 State=UP Default=NO DefaultTime=01:00:00

EnforcePartLimits=ALL

## scheduler settings
#
SchedulerType=sched/backfill
SchedulerPort=7321
SelectType=select/cons_res
SelectTypeParameters=CR_Core_Memory
FastSchedule=1
## raapoi defaults
DefMemPerCPU=1000

# use the "multifactor" plugin with weights set up to be multi-user ready
PriorityType=priority/multifactor
PriorityWeightAge=12500
PriorityWeightFairshare=100000
PriorityWeightJobSize=12500
PriorityWeightPartition=0
PriorityWeightQOS=20000

# fair share settings
PriorityDecayHalfLife=14-0
PriorityUsageResetPeriod=NONE
PriorityMaxAge=7-0
PriorityFavorSmall=NO


## accounting settings
#

AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageEnforce=associations,limits,qos
AccountingStoreJobComment=YES
AccountingStorageHost=raapoi-master
AccountingStoragePort=6819

# the "job completion" info is redundant if the accounting
# infrastructure is enabled, so turn it off as it's an endless source
# of authentication and DB connection problems ...
JobCompType=jobcomp/none

# No power consumption acct
AcctGatherEnergyType=acct_gather_energy/none

# No IB usage accounting
AcctGatherInfinibandType=acct_gather_infiniband/none

# No filesystem accounting (only works with Lustre)
AcctGatherFilesystemType=acct_gather_filesystem/none

# No job profiling (for now)
AcctGatherProfileType=acct_gather_profile/none
#AcctGatherProfileType=acct_gather_profile/hdf5

JobAcctGatherType=jobacct_gather/linux
JobAcctGatherFrequency=60


## resource scheduling (GRES)
#
GresTypes=gpu


## job execution settings
#

CheckpointType=checkpoint/none
#CheckpointType=checkpoint/blcr
JobCheckpointDir=/var/lib/slurm/checkpoint

# requeue jobs on node failure, unless users ask otherwise
JobRequeue=1

# max number of jobs in a job array
MaxArraySize=4000001

# max number of jobs pending + running
MaxJobCount=10000


MpiDefault=openmpi
# Note: Apparently, the `MpiParams` option is needed also for non-mpi
# jobs in slurm 2.5.3.
MpiParams=ports=12000-12999

# track resource usage via Linux /proc tree
ProctrackType=proctrack/cgroup

# do not propagate `ulimit` restrictions found on login nodes
PropagateResourceLimits=NONE

# automatically return nodes to service, unless they have been marked DOWN by admins
ReturnToService=2

TaskPlugin=task/none
#TaskEpilog=/etc/slurm/task_epilog
#TaskProlog=/etc/slurm/task_prolog

TmpFs=/tmp

# limit virtual mem usage to this% of real mem usage
VSizeFactor=101


# misc timeout settings (commented lines show the default)
#
BatchStartTimeout=60
CompleteWait=35
#EpilogMsgTime=2000
#HealthCheckInterval=0
#HealthCheckProgram=
InactiveLimit=0
KillWait=30
#MessageTimeout=10
#ResvOverRun=0
MinJobAge=300
#OverTimeLimit=0
#UnkillableStepTimeout=60
#VSizeFactor=0
Waittime=0


## `slurmctld` settings (controller nodes)
#
ControlMachine=raapoi-master
ControlAddr=130.195.19.14

SlurmctldPidFile=/var/run/slurmctld.pid
SlurmctldPort=6817
SlurmctldTimeout=300

StateSaveLocation=/var/spool/slurm

SlurmctldDebug=error
SlurmctldLogFile=/var/log/slurm/slurmctld.log
DebugFlags=backfill,cpu_bind,priority,reservation,selecttype,steps,NO_CONF_HASH

MailProg=/usr/bin/mail


## `slurmd` settings (compute nodes)
#
SlurmdPort=6818
SlurmdPidFile=/var/run/slurmd.pid
SlurmdSpoolDir=/var/lib/slurm/slurmd
SlurmdTimeout=300

SlurmdDebug=error
SlurmdLogFile=/var/log/slurm/slurmd.log

AuthType=auth/munge
CryptoType=crypto/munge

DisableRootJobs=NO
Epilog=/home/software/tools/slurm/epilog/*

PrologFlags=x11
